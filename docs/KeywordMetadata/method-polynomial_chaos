Blurb:: Uncertainty quantification using polynomial chaos expansions

Description:: 
The polynomial chaos expansion (PCE) is a general framework for
the approximate representation of random response functions in terms
of finite-dimensional series expansions in standardized random variables

\f[R = \sum_{i=0}^P \alpha_i \Psi_i(\xi) \f]

where \f$\alpha_i\f$ is a deterministic coefficient, \f$\Psi_i\f$ is a
multidimensional orthogonal polynomial and \f$\xi\f$ is a vector of
standardized random variables. An important distinguishing feature of
the methodology is that the functional relationship between random
inputs and outputs is captured, not merely the output statistics as in
the case of many nondeterministic methodologies.

<!--
Groups 1 and 2, plus the optional keywords \c p_refinement and
\c fixed_seed relate 
to the specification of a PCE method. In addition, this method
treats variables that are not aleatoric-uncertain different,
despite the \ref variables-active keyword.

Group 3, and the remainder of the optional keywords relate 
to the output of the method.
-->

<b> Basis polynomial family (Group 1) </b>

Group 1 keywords are used to select the type of basis,
\f$\Psi_i\f$, of the expansion. Three approaches may be employed:

\li Wiener: employs standard normal random variables in a transformed
  probability space, corresponding to Hermite orthogonal basis
  polynomials (see \ref method-polynomial_chaos-wiener).

\li Askey: employs standard normal, standard uniform, standard
  exponential, standard beta, and standard gamma random variables in a
  transformed probability space, corresponding to Hermite, Legendre,
  Laguerre, Jacobi, and generalized Laguerre orthogonal basis
  polynomials, respectively (see \ref method-polynomial_chaos-askey).

\li Extended (default if no option is selected): The Extended option
  avoids the use of any nonlinear variable transformations by
  augmenting the Askey approach with numerically-generated orthogonal
  polynomials for non-Askey probability density functions.  Extended
  polynomial selections replace each of the sub-optimal Askey basis
  selections <!-- with numerically-generated polynomials that are
  orthogonal to the prescribed probability density functions -->
  for bounded normal, lognormal, bounded lognormal, loguniform,
  triangular, gumbel, frechet, weibull, and bin-based histogram.

For supporting correlated random variables, certain fallbacks must
be implemented.  <!-- The selection of Wiener versus Askey versus 
Extended is partially automated and partially under the user's control. -->
- The Extended option is the default and supports only
  Gaussian correlations. <!--- This default can
  be overridden by the user by supplying the keyword \c askey to request
  restriction to the use of Askey bases only or by supplying the keyword
  \c wiener to request restriction to the use of exclusively Hermite
  bases. --> 
- If needed to support prescribed correlations (not under user
  control), the Extended and Askey options will fall back to the Wiener
  option <EM>on a per variable basis</EM>. If the prescribed
  correlations are also unsupported by Wiener expansions, then %Dakota
  will exit with an error.

Refer to \ref topic-variable_support for additional information on
supported variable types, with and without correlation.

<b> Coefficient estimation approach (Group 2) </b>

To obtain the coefficients \f$\alpha_i\f$ of the expansion, seven
options are provided:

<ol>
<li> multidimensional integration by a tensor-product of Gaussian
     quadrature rules (specified with \c quadrature_order, and,
     optionally, \c dimension_preference).
<li> multidimensional integration by the Smolyak sparse grid method
     (specified with \c sparse_grid_level and, optionally, 
     \c dimension_preference)
<li> multidimensional integration by Stroud cubature rules 
     and extensions as specified with \c cubature_integrand. 
<li> multidimensional integration by Latin hypercube sampling
     (specified with \c expansion_order and \c expansion_samples).
<li> linear regression (specified with \c expansion_order and 
     either \c collocation_points or \c collocation_ratio), using 
     either over-determined (least squares) or under-determined 
     (compressed sensing) approaches.
<li> orthogonal least interpolation (specified with 
     \c orthogonal_least_interpolation and \c collocation_points)
<li> coefficient import from a file (specified with \c
     import_expansion_file). The expansion can be comprised of a 
     general set of expansion terms, as indicated by the multi-index 
     annotation within the file.
</ol>

It is important to note that, for polynomial chaos using a single
model fidelity, \c quadrature_order, \c sparse_grid_level, and \c
expansion_order are scalar inputs used for a single expansion
estimation.  These scalars can be augmented with a \c
dimension_preference to support anisotropy across the random dimension
set.  This differs from the use of sequence arrays in advanced use
cases such as multilevel and multifidelity polynomial chaos, where
multiple grid resolutions can be employed across a model hierarchy.

<b> Active Variables </b>

The default behavior is to form expansions over aleatory 
uncertain continuous variables. To form expansions 
over a broader set of variables, one needs to specify 
\c active followed by \c state, \c epistemic, \c design, or \c all 
in the variables specification block. 

For continuous design, continuous state, and continuous
epistemic uncertain variables included in the expansion,
Legendre chaos bases are used to model the bounded intervals for these
variables. However, these variables are not assumed to have any
particular probability distribution, only that they are independent
variables. Moreover, when probability integrals are evaluated, only
the aleatory random variable domain is integrated, leaving behind a
polynomial relationship between the statistics and the remaining
design/state/epistemic variables.

<b> Covariance type (Group 3) </b>

These two keywords are used to specify how this method computes, stores,
and outputs the covariance of the responses.  In particular, the diagonal
covariance option is provided for reducing post-processing overhead and 
output volume in high dimensional applications.

<b> Optional Keywords regarding method outputs </b>

Each of these sampling specifications refer to sampling on the PCE
approximation for the purposes of generating approximate statistics.
\li \c sample_type
\li \c samples
\li \c seed
\li \c fixed_seed
\li \c rng
\li \c probability_refinement 
\li \c distribution
\li \c reliability_levels
\li \c response_levels
\li \c probability_levels
\li \c gen_reliability_levels

which should be distinguished from simulation sampling for generating
the PCE coefficients as described in options 4, 5, and 6 above
(although these options will share the \c sample_type, \c seed, and \c
rng settings, if provided).

When using the \c probability_refinement control, the number of
refinement samples is not under the user's control (these evaluations
are approximation-based, so management of this expense is less
critical). This option allows for refinement of probability and
generalized reliability results using importance sampling.


<b> Usage Tips </b>

If \e n is small (e.g., two or three), then tensor-product Gaussian
quadrature is quite effective and can be the preferred choice. For
moderate to large \e n (e.g., five or more), tensor-product quadrature
quickly becomes too expensive and the sparse grid and regression
approaches are preferred. <!-- For large \e n (e.g., more than ten),
point collocation may begin to suffer from ill-conditioning and sparse
grids are generally recommended. --> Random sampling for coefficient
estimation is generally not recommended due to its slow convergence
rate. <!--, although it does hold the advantage that the simulation
budget is more flexible than that required by the other approaches.-->
For incremental studies, approaches 4 and 5 support reuse of previous
samples through the \ref method-sampling-sample_type-incremental_lhs 
and \ref model-surrogate-global-reuse_points
specifications, respectively.

In the quadrature and sparse grid cases, growth rates for nested and
non-nested rules can be synchronized for consistency. For a
non-nested Gauss rule used within a sparse grid, linear
one-dimensional growth rules of \f$m=2l+1\f$ are used to enforce odd
quadrature orders, where \e l is the grid level and \e m is the number
of points in the rule. The precision of this Gauss rule is then
\f$i=2m-1=4l+1\f$. For nested rules, order growth with level is
typically exponential; however, the default behavior is to restrict
the number of points to be the lowest order rule that is available
that meets the one-dimensional precision requirement implied by either
a level \e l for a sparse grid (\f$i=4l+1\f$) or an order \e m for a
tensor grid (\f$i=2m-1\f$). This behavior is known as "restricted
growth" or "delayed sequences." To override this default behavior in
the case of sparse grids, the \c unrestricted keyword can be used; it
cannot be overridden for tensor grids using nested rules since it also
provides a mapping to the available nested rule quadrature orders. An
exception to the default usage of restricted growth is the \c
dimension_adaptive \c p_refinement \c generalized sparse grid case
described previously, since the ability to evolve the index sets of a
sparse grid in an unstructured manner eliminates the motivation for
restricting the exponential growth of nested rules.

<b> Additional Resources </b>

%Dakota provides access to PCE methods through the NonDPolynomialChaos
class. Refer to the Uncertainty Quantification Capabilities chapter of
the Users Manual \cite UsersMan and the Stochastic Expansion Methods
chapter of the Theory Manual \cite TheoMan for additional information
on the PCE algorithm.

<b> Expected HDF5 Output </b>

If Dakota was built with HDF5 support and run with the 
\ref environment-results_output-hdf5 keyword, this method
writes the following results to HDF5:

- \ref hdf5_results-se_moments
- \ref hdf5_results-pdf
- \ref hdf5_results-level_mappings
- \ref hdf5_results-vbd



Topics::

Examples::
\verbatim
method,
	polynomial_chaos
	  sparse_grid_level = 2	
	  samples = 10000 seed = 12347 rng rnum2	
	  response_levels = .1 1. 50. 100. 500. 1000.	
	  variance_based_decomp
\endverbatim

Theory::

Faq::
See_Also::	method-stoch_collocation, method-function_train
