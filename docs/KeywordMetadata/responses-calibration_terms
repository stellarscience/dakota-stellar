Blurb::
Response type suitable for calibration or least squares

Description::
Responses for a calibration study are specified using \c
calibration_terms and optional keywords for weighting/scaling, data,
and constraints.  In general when calibrating, Dakota automatically
tunes parameters \f$ \theta \f$ to minimize discrepancies or residuals
between the model and the data:

\f[ R_{i} = y^{Model}_i(\theta) - y^{Data}_{i}. \f]

Note that the problem specification affects what must be returned to
Dakota in the \ref interface-analysis_drivers-fork-results_file :

\li If calibration data <em>is not specified</em>, then each of the
  calibration terms returned to Dakota through the \ref interface is a
  residual \f$ R_{i} \f$ to be driven toward zero.

\li If calibration data <em>is specified</em>, then each of the
  calibration terms returned to Dakota must be a response \f$
  y^{Model}_i(\theta) \f$, which Dakota will difference with the data
  in the specified data file.

<b> Constraints </b>

(See general problem formulation at \ref
responses-objective_functions.) The keywords \ref
responses-calibration_terms-nonlinear_inequality_constraints and \ref
responses-calibration_terms-nonlinear_equality_constraints specify the
number of nonlinear inequality constraints \em g, and nonlinear
equality constraints \em h, respectively.  When interfacing to
external applications, the responses must be returned to %Dakota in
this order in the \ref interface-analysis_drivers-fork-results_file :
<ol> <li>calibration terms</li> <li>nonlinear inequality
constraints</li> <li>nonlinear equality constraints</li> </ol>

An optimization problem's linear constraints are provided to the
solver at startup only and do not need to be included in the data
returned on every function evaluation. Linear constraints are
therefore specified in the \ref variables block through the \ref
variables-linear_inequality_constraint_matrix \f$A_i\f$ and \ref
variables-linear_equality_constraint_matrix \f$A_e\f$.

Lower and upper bounds on the design variables \em x are also
specified in the \ref variables block.

<b> Problem Transformations</b>

Weighting or scaling calibration terms is often appropriate to account
for measurement error or to condition the problem for easier solution.
Weighting or scaling transformations are applied in the following
order:

<ol>
<li> When present, observation error variance \f$ \sigma_i \f$ or full
     covariance \f$ \Sigma\f$, optionally specified through \c
     experiment_variance_type, is applied to residuals first: 
     
     \f[  R^{(1)}_i = \frac{R_{i}}{\sigma_{i}} = \frac{y^{Model}_i(\theta) -
     y^{Data}_{i}}{\sigma_{i}}  \textrm{, or} \f]
 
     \f[
     R^{(1)} = \Sigma^{-1/2} R = \Sigma^{-1/2} \left(y^{Model}(\theta) -
     y^{Data}\right), \f]
     resulting in the typical variance-weighted least squares formulation
     \f[ \textrm{min}_\theta \; R(\theta)^T \Sigma^{-1} R(\theta) \f]
</li>
<li> Any active scaling transformations are applied next, e.g., for
     characteristic value scaling:

     \f[ R^{(2)}_i = \frac{R^{(1)}_i}{s_i} \f]
</li>
<li> Finally the optional weights are applied in a way that preserves
    backward compatibility:

    \f[ R^{(3)}_i = \sqrt{w_i}{R^{(2)}_i} \f]

    so the ultimate least squares formulation, e.g., in a scaled and
    weighted case would be

    \f[ f = \sum_{i=1}^{n} w_i \left( \frac{y^{Model}_i -
    y^{Data}_i}{s_i} \right)^2 \f]
</li>
</ol>

<em>Note that specifying observation error variance and weights are mutually
exclusive in a calibration problem.</em>

Topics::	
Examples::
Theory::

%Dakota calibration terms are typically used to solve problems of
parameter estimation, system identification, and model
calibration/inversion. Local least squares calibration problems are
most efficiently solved using special-purpose least squares solvers
such as Gauss-Newton or Levenberg-Marquardt; however, they may also be
solved using any general-purpose optimization algorithm in %Dakota.
While Dakota can solve these problems with either least squares or
optimization algorithms, the response data sets to be returned from
the simulator are different when using \ref
responses-objective_functions versus \ref responses-calibration_terms.

Least squares calibration involves a set of residual
functions, whereas optimization involves a single objective function
(sum of the squares of the residuals), i.e., \f[f = \sum_{i=1}^{n}
R_i^2 = \sum_{i=1}^{n} \left(y^{Model}_i(\theta) - y^{Data}_{i} \right)^2 \f] 
where \e f is the objective function and the set of \f$R_i\f$
are the residual functions, most commonly defined as the difference between a model response and data. Therefore, function values and derivative
data in the least squares case involve the values and derivatives of
the residual functions, whereas the optimization case involves values
and derivatives of the sum of squares objective function. This means that 
in the least squares calibration case, the user must return each of 
\c n residuals 
separately as a separate calibration term. Switching
between the two approaches sometimes requires different simulation
interfaces capable of returning the different granularity of response
data required, although %Dakota supports automatic recasting of
residuals into a sum of squares for presentation to an optimization
method. Typically, the user must compute the difference between the 
model results and the observations when computing the residuals. 
However, the user has the option of specifying the observational data 
(e.g. from physical experiments or other sources) in a file. 

Faq::
See_Also::	responses-objective_functions, responses-response_functions
